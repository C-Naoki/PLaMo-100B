# PLaMo-100B
- [â„¹ï¸ About](#â„¹ï¸-about)
- [ğŸ§ What is PLaMo-100B?](#-what-is-plamo-100b)
  - [What is the post-training process?](#what-is-the-post-training-process)
- [ğŸ™ Acknowledgement](#-acknowledgement)
- [ğŸ™‹â€â™‚ï¸ Support](#ï¸-support)
- [ğŸ“— References](#-references)

## â„¹ï¸ About
This repository is an unofficial Python implementation to demonstrate PLaMo-100B.
If you want to quickly try its demo, let's visit the [website](https://plamo100b-demo.streamlit.app/)! (Please note that the trial API for the website is available for a limited time only. After the trial period ends, access to the API will no longer be available. I appreciate your understanding.)

## ğŸ§ What is PLaMo-100B?
PLaMo-100B is the Large Language Model developed in-house by [Preferred Elements](https://www.preferred.jp/en/) (a subsidiary of [Preferred Networks](https://www.preferred.jp/en/)).
This model has been developed since Februrary 2024, and the post-training process has just been completed on August 7, 2024.
The 100B in the model name stands for the number of parameters, namely <u>**100 billion**</u>.

PLaMo-100B-Instruct, where the model post-training is completed, outperformed the GPT-4 model on [Jaster](https://github.com/llm-jp/llm-jp-eval/tree/g-leaderboard). Note that Jaster is a benchmark dataset for Japanese language models. Additionally, PLaMo-100B-Instruct also achieved a higher score than the GPT-4 model on [Rakuda benchmark](https://github.com/yuzu-ai/japanese-llm-ranking), which is used to evaluate how well models can handle insights into the nuances of Japanese language.

### What is the post-training process?
The post-training process is a process to further train the model using a large amount of data after the initial training. The post-training process is used to improve the performance of the model on specific tasks or datasets.

## ğŸ™ Acknowledgement
I appreciate that Preferred Networks offered the trial API to access PLaMo-100B model.

## ğŸ™‹â€â™‚ï¸ Support
ğŸ’™ If you like this app, give it a â­ and share it with friends!

## ğŸ“— References
[1] [1,000å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç‹¬è‡ªLLMã€ŒPLaMo-100Bã€ã®äº‹å¾Œå­¦ç¿’ãŒå®Œäº†](https://tech.preferred.jp/ja/blog/plamo-100b-post-training/) \
[2] [PFEãŒé–‹ç™ºã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«PLaMo Î²ç‰ˆã®ç„¡æ–™ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã®ç”³è¾¼å—ä»˜ã‚’é–‹å§‹](https://www.preferred.jp/ja/news/pr20240807/)
